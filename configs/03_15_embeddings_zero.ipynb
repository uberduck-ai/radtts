{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c665728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wtf happened to this file and why doesn't the torch dataloader find it?\n",
    "# audiopath = \"/usr/src/app/radtts/data/lj_data/LJSpeech-1.1/wavs/LJ027-0028_22k_normalized.wav\"\n",
    "# pitchpath = \"/usr/src/app/radtts/data_cache/lj_data_LJSpeech-1.1_wavs_LJ027-0028_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\"\n",
    "\n",
    "\n",
    "# audiopath = \"/usr/src/app/radtts/data/lj_data/LJSpeech-1.1/wavs/LJ044-0174_22k_normalized.wav\"\n",
    "# pitchpath = \"/usr/src/app/radtts/data_cache/lj_data_LJSpeech-1.1_wavs_LJ044-0174_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\"\n",
    "import os\n",
    "use_log_f0 = 0\n",
    "audiopath = \"/usr/src/app/radtts/data/lj_data/LJSpeech-1.1/wavs/LJ003-0239_22k_normalized.wav\"\n",
    "# pitchpath = \"/usr/src/app/radtts/data_cache/lj_data_LJSpeech-1.1_wavs_LJ003-0239_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\"\n",
    "filename = '_'.join(audiopath.split('/')[-4:])\n",
    "betabinom_cache_path = \"/usr/src/app/radtts/data_cache\"\n",
    "f0_path = os.path.join(betabinom_cache_path, filename)\n",
    "f0_path += \"_f0_sr{}_fl{}_hl{}_f0min{}_f0max{}_log{}.pt\".format(\n",
    "    sampling_rate, filter_length, hop_length,\n",
    "    data_config['f0_min'], data_config['f0_max'], use_log_f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7dd9c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/usr/src/app/radtts/data/lj_data/LJSpeech-1.1/metadata_formatted_full_pitch.txt', sep = '|', index_col = None,header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "479db5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/src/app/radtts/data_cache/lj_data_LJSpeech-1.1_wavs_LJ005-0094_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "/usr/src/app/radtts/data_cache/lj_data_LJSpeech-1.1_wavs_LJ040-0007_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n"
     ]
    }
   ],
   "source": [
    "audiopaths = data[0]\n",
    "for audiopath in audiopaths:\n",
    "    \n",
    "    filename = '_'.join(audiopath.split('/')[-4:])\n",
    "    betabinom_cache_path = \"/usr/src/app/radtts/data_cache\"\n",
    "    f0_path = os.path.join(betabinom_cache_path, filename)\n",
    "    f0_path += \"_f0_sr{}_fl{}_hl{}_f0min{}_f0max{}_log{}.pt\".format(\n",
    "        sampling_rate, filter_length, hop_length,\n",
    "        data_config['f0_min'], data_config['f0_max'], use_log_f0)    \n",
    "    \n",
    "    if not os.path.exists(f0_path):\n",
    "        print(f0_path)\n",
    "        audio, sampling_rate = load_wav_to_torch(audiopath)\n",
    "        f0, voiced_mask, p_voiced = get_f0_pvoiced(audio.cpu().numpy(), sampling_rate=sampling_rate, frame_length=filter_length,\n",
    "                           hop_length=hop_length, f0_min=mel_fmin, f0_max=mel_fmax)\n",
    "        torch.save({'f0': f0,\n",
    "                    'voiced_mask': voiced_mask,\n",
    "                    'p_voiced': p_voiced}, f0_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb45d7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdab14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4311b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/src/app/radtts/data_cache/lj_data_LJSpeech-1.1_wavs_LJ003-0239_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f0_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "438b0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from librosa import pyin\n",
    "from uberduck_ml_dev.exec.train_radtts_with_ray import data_config, load_wav_to_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b556290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_length=data_config['filter_length']\n",
    "hop_length=data_config['hop_length']\n",
    "win_length=data_config['win_length']\n",
    "sampling_rate=22050\n",
    "n_mel_channels=data_config['n_mel_channels']\n",
    "f0_min=data_config['f0_min']\n",
    "f0_max=data_config['f0_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c033ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_wav_value = 32768.\n",
    "\n",
    "def get_f0_pvoiced(audio, sampling_rate=22050, frame_length=1024,\n",
    "                   hop_length=256, f0_min=100, f0_max=300):\n",
    "\n",
    "    audio_norm = audio / max_wav_value\n",
    "    f0, voiced_mask, p_voiced = pyin(\n",
    "        audio_norm, f0_min, f0_max, sampling_rate,\n",
    "        frame_length=frame_length, win_length=frame_length // 2,\n",
    "        hop_length=hop_length)\n",
    "    f0[~voiced_mask] = 0.0\n",
    "    f0 = torch.FloatTensor(f0)\n",
    "    p_voiced = torch.FloatTensor(p_voiced)\n",
    "    voiced_mask = torch.FloatTensor(voiced_mask)\n",
    "    return f0, voiced_mask, p_voiced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9b61d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "audio, sampling_rate = load_wav_to_torch(audiopath)\n",
    "f0, voiced_mask, p_voiced = get_f0_pvoiced(audio.cpu().numpy(), sampling_rate=sampling_rate, frame_length=filter_length,\n",
    "                   hop_length=hop_length, f0_min=f0_min, f0_max=f0_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9c2fdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'f0': f0,\n",
    "            'voiced_mask': voiced_mask,\n",
    "            'p_voiced': p_voiced}, pitchpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d137af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22050"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d7b1de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pyin in module librosa.core.pitch:\n",
      "\n",
      "pyin(y, fmin, fmax, sr=22050, frame_length=2048, win_length=None, hop_length=None, n_thresholds=100, beta_parameters=(2, 18), boltzmann_parameter=2, resolution=0.1, max_transition_rate=35.92, switch_prob=0.01, no_trough_prob=0.01, fill_na=nan, center=True, pad_mode='reflect')\n",
      "    Fundamental frequency (F0) estimation using probabilistic YIN (pYIN).\n",
      "    \n",
      "    pYIN [#]_ is a modificatin of the YIN algorithm [#]_ for fundamental frequency (F0) estimation.\n",
      "    In the first step of pYIN, F0 candidates and their probabilities are computed using the YIN algorithm.\n",
      "    In the second step, Viterbi decoding is used to estimate the most likely F0 sequence and voicing flags.\n",
      "    \n",
      "    .. [#] Mauch, Matthias, and Simon Dixon.\n",
      "        \"pYIN: A fundamental frequency estimator using probabilistic threshold distributions.\"\n",
      "        2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014.\n",
      "    \n",
      "    .. [#] De CheveignÃ©, Alain, and Hideki Kawahara.\n",
      "        \"YIN, a fundamental frequency estimator for speech and music.\"\n",
      "        The Journal of the Acoustical Society of America 111.4 (2002): 1917-1930.\n",
      "    \n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y : np.ndarray [shape=(n,)]\n",
      "        audio time series.\n",
      "    \n",
      "    fmin: number > 0 [scalar]\n",
      "        minimum frequency in Hertz.\n",
      "        The recomended minimum is ``librosa.note_to_hz('C2')`` (~65 Hz)\n",
      "        though lower values may be feasible.\n",
      "    \n",
      "    fmax: number > 0 [scalar]\n",
      "        maximum frequency in Hertz.\n",
      "        The recomended maximum is ``librosa.note_to_hz('C7')`` (~2093 Hz)\n",
      "        though higher values may be feasible.\n",
      "    \n",
      "    sr : number > 0 [scalar]\n",
      "        sampling rate of ``y`` in Hertz.\n",
      "    \n",
      "    frame_length : int > 0 [scalar]\n",
      "         length of the frames in samples.\n",
      "         By default, ``frame_length=2048`` corresponds to a time scale of about 93 ms at\n",
      "         a sampling rate of 22050 Hz.\n",
      "    \n",
      "    win_length : None or int > 0 [scalar]\n",
      "        length of the window for calculating autocorrelation in samples.\n",
      "        If ``None``, defaults to ``frame_length // 2``\n",
      "    \n",
      "    hop_length : None or int > 0 [scalar]\n",
      "        number of audio samples between adjacent pYIN predictions.\n",
      "        If ``None``, defaults to ``frame_length // 4``.\n",
      "    \n",
      "    n_thresholds : int > 0 [scalar]\n",
      "        number of thresholds for peak estimation.\n",
      "    \n",
      "    beta_parameters : tuple\n",
      "        shape parameters for the beta distribution prior over thresholds.\n",
      "    \n",
      "    boltzmann_parameter: number > 0 [scalar]\n",
      "        shape parameter for the Boltzmann distribution prior over troughs.\n",
      "        Larger values will assign more mass to smaller periods.\n",
      "    \n",
      "    resolution : float in `(0, 1)`\n",
      "        Resolution of the pitch bins.\n",
      "        0.01 corresponds to cents.\n",
      "    \n",
      "    max_transition_rate : float > 0\n",
      "        maximum pitch transition rate in octaves per second.\n",
      "    \n",
      "    switch_prob : float in ``(0, 1)``\n",
      "        probability of switching from voiced to unvoiced or vice versa.\n",
      "    \n",
      "    no_trough_prob : float in ``(0, 1)``\n",
      "        maximum probability to add to global minimum if no trough is below threshold.\n",
      "    \n",
      "    fill_na : None, float, or ``np.nan``\n",
      "        default value for unvoiced frames of ``f0``.\n",
      "        If ``None``, the unvoiced frames will contain a best guess value.\n",
      "    \n",
      "    center : boolean\n",
      "        If ``True``, the signal ``y`` is padded so that frame\n",
      "        ``D[:, t]`` is centered at ``y[t * hop_length]``.\n",
      "        If ``False``, then ``D[:, t]`` begins at ``y[t * hop_length]``.\n",
      "        Defaults to ``True``,  which simplifies the alignment of ``D`` onto a\n",
      "        time grid by means of ``librosa.core.frames_to_samples``.\n",
      "    \n",
      "    pad_mode : string or function\n",
      "        If ``center=True``, this argument is passed to ``np.pad`` for padding\n",
      "        the edges of the signal ``y``. By default (``pad_mode=\"reflect\"``),\n",
      "        ``y`` is padded on both sides with its own reflection, mirrored around\n",
      "        its first and last sample respectively.\n",
      "        If ``center=False``,  this argument is ignored.\n",
      "        .. see also:: `np.pad`\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    f0: np.ndarray [shape=(n_frames,)]\n",
      "        time series of fundamental frequencies in Hertz.\n",
      "    \n",
      "    voiced_flag: np.ndarray [shape=(n_frames,)]\n",
      "        time series containing boolean flags indicating whether a frame is voiced or not.\n",
      "    \n",
      "    voiced_prob: np.ndarray [shape=(n_frames,)]\n",
      "        time series containing the probability that a frame is voiced.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    librosa.yin\n",
      "        Fundamental frequency (F0) estimation using the YIN algorithm.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Computing a fundamental frequency (F0) curve from an audio input\n",
      "    \n",
      "    >>> y, sr = librosa.load(librosa.ex('trumpet'))\n",
      "    >>> f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
      "    >>> times = librosa.times_like(f0)\n",
      "    \n",
      "    \n",
      "    Overlay F0 over a spectrogram\n",
      "    \n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
      "    >>> fig, ax = plt.subplots()\n",
      "    >>> img = librosa.display.specshow(D, x_axis='time', y_axis='log', ax=ax)\n",
      "    >>> ax.set(title='pYIN fundamental frequency estimation')\n",
      "    >>> fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
      "    >>> ax.plot(times, f0, label='f0', color='cyan', linewidth=3)\n",
      "    >>> ax.legend(loc='upper right')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pyin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25910ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
