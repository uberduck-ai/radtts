{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69db9a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "from torch.cuda import amp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "# import radtts\n",
    "import os\n",
    "import json\n",
    "from timeit import default_timer as timer\n",
    "os.chdir('/usr/src/app/radtts')\n",
    "from distributed import (init_distributed, apply_gradient_allreduce,\n",
    "                         reduce_tensor)\n",
    "\n",
    "from radtts import RADTTS\n",
    "from train import RADTTSLoss, AttentionBinarizationLoss, prepare_dataloaders, prepare_model_weights, parse_data_from_batch, compute_validation_loss\n",
    "import torch\n",
    "# from radtts.inference import load_vocoder\n",
    "from hifigan_env import AttrDict\n",
    "from data import Data\n",
    "from hifigan_models import Generator\n",
    "from hifigan_env import AttrDict\n",
    "from hifigan_denoiser import Denoiser\n",
    "from radam import RAdam\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "# Needs to be here cause of implicit \"config\" argument\n",
    "def prepare_output_folders_and_logger(output_directory):\n",
    "    # Get shared output_directory ready\n",
    "    if not os.path.isdir(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "        os.chmod(output_directory, 0o775)\n",
    "        print(\"output directory\", output_directory)\n",
    "\n",
    "    output_config_path = os.path.join(output_directory, 'config.json')\n",
    "    print(\"saving current configuration in output dir\")\n",
    "    config_fp = open(output_config_path, 'w')\n",
    "    json.dump(config, config_fp, indent=4)\n",
    "    config_fp.close()\n",
    "    output_code_path = os.path.join(output_directory, 'code.tar.gz')\n",
    "    os.system('tar -czvf %s *.py' % (output_code_path))\n",
    "\n",
    "    tboard_out_path = os.path.join(output_directory, 'logs')\n",
    "    print(\"setting up tboard log in %s\" % (tboard_out_path))\n",
    "    logger = SummaryWriter(tboard_out_path)\n",
    "    return logger\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9eb2b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying spectral norm to text encoder LSTM\n",
      "Applying spectral norm to context encoder LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/src/app/radtts/common.py:391: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "Q, R = torch.qr(A, some)\n",
      "should be replaced with\n",
      "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2349.)\n",
      "  W = torch.qr(torch.FloatTensor(c, c).normal_())[0]\n",
      "/opt/conda/lib/python3.8/site-packages/torch/functional.py:1682: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.\n",
      "LU, pivots = torch.lu(A, compute_pivots)\n",
      "should be replaced with\n",
      "LU, pivots = torch.linalg.lu_factor(A, compute_pivots)\n",
      "and\n",
      "LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)\n",
      "should be replaced with\n",
      "LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1915.)\n",
      "  return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))\n"
     ]
    }
   ],
   "source": [
    "config_path = '/usr/src/app/radtts/configs/2_22_23.json'\n",
    "with open(config_path) as f:\n",
    "    config = json.load(f)\n",
    "seed = 1234\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "n_gpus = 1\n",
    "train_config = config['train_config']\n",
    "sigma = train_config['sigma']\n",
    "model_config = config['model_config']\n",
    "loss_weights = train_config['loss_weights']\n",
    "learning_rate = train_config['learning_rate']\n",
    "unfreeze_modules = train_config['unfreeze_modules']\n",
    "weight_decay = train_config['weight_decay']\n",
    "output_directory = train_config['output_directory']\n",
    "use_amp = train_config['use_amp']\n",
    "batch_size = 4\n",
    "iters_per_checkpoint = train_config['iters_per_checkpoint']\n",
    "grad_clip_val = train_config['grad_clip_val']\n",
    "data_config = config['data_config']\n",
    "epochs = train_config['epochs']\n",
    "binarization_start_iter = train_config['binarization_start_iter']\n",
    "criterion = RADTTSLoss(\n",
    "    sigma,\n",
    "    model_config['n_group_size'],\n",
    "    model_config['dur_model_config'],\n",
    "    model_config['f0_model_config'],\n",
    "    model_config['energy_model_config'],\n",
    "    vpred_model_config=model_config['v_model_config'],\n",
    "    loss_weights=loss_weights\n",
    ")\n",
    "attention_kl_loss = AttentionBinarizationLoss()\n",
    "model = RADTTS(**model_config).cuda()\n",
    "rank = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2baa3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing training dataloader\n",
      "Number of speakers: 29\n",
      "Speaker IDS {'0': 0, '1': 1, '10': 2, '11': 3, '12': 4, '13': 5, '14': 6, '15': 7, '16': 8, '17': 9, '19': 10, '2': 11, '20': 12, '21': 13, '22': 14, '23': 15, '24': 16, '25': 17, '26': 18, '27': 19, '28': 20, '29': 21, '3': 22, '4': 23, '5': 24, '6': 25, '7': 26, '8': 27, '9': 28}\n",
      "Number of files 21584\n",
      "Number of files after duration filtering 21584\n",
      "Dataloader initialized with no augmentations\n",
      "initializing validation dataloader\n",
      "Number of files 21584\n",
      "Number of files after duration filtering 21584\n",
      "Dataloader initialized with no augmentations\n",
      "saving current configuration in output dir\n",
      "setting up tboard log in /usr/src/app/radtts/outputs/2_22_23/logs\n",
      "Training everything\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "optimizer = RAdam(model.parameters(), lr=learning_rate,\n",
    "                      weight_decay=weight_decay)\n",
    "\n",
    "scaler = amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = learning_rate\n",
    "\n",
    "train_loader, valset, collate_fn = prepare_dataloaders(\n",
    "    data_config, n_gpus, batch_size)\n",
    "\n",
    "logger = prepare_output_folders_and_logger(output_directory)\n",
    "\n",
    "prepare_model_weights(model, unfreeze_modules)\n",
    "model.train()\n",
    "\n",
    "epoch_offset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b2f19bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "saving f0 to data_cache/sam-lachow_wavs_91_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/Carolyn_Speaking_wavs_220_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ015-0184_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ006-0045_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/dataset_wavs_174_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/dataset_wavs_178_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ003-0122_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ016-0096_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ022-0118_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ003-0131_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ024-0134_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ010-0299_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/sam-lachow_wavs_46_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ045-0070_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ003-0145_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ031-0001_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ016-0294_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ045-0009_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ016-0004_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ042-0043_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ038-0181_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ004-0208_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ047-0108_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ018-0282_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ025-0131_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ003-0277_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ003-0347_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ010-0215_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/dataset_wavs_128_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ008-0225_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ025-0009_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ033-0009_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ011-0240_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ017-0221_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ031-0015_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ013-0180_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ015-0032_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "0\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ025-0005_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/Po_Data_wavs_55_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ017-0144_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ012-0069_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ006-0162_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/TAG_wavs_208_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.ptsaving f0 to data_cache/jr_base_wavs_195_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ048-0039_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ023-0033_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ010-0274_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/dataset_wavs_243_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/wrl_wavs_0001_df0ad772-8a5e-41c6-a084-ddc9e28f8164_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/sjk_wavs_0167_7f46f51a-8c02-4b54-bc94-14a28c9bcd44_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ010-0208_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ009-0264_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/JRodriguesOldTimerDataset_wavs_79_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ050-0086_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ016-0207_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ014-0296_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n",
      "saving f0 to data_cache/LJSpeech-1.1_wavs_LJ039-0056_22k_normalized.wav_f0_sr22050_fl1024_hl256_f0min80.0_f0max640.0_log0.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_INTERNAL_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-dc585b3905a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_amp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             outputs = model(\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mmel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_lens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mbinarize_attention\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_prior\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/src/app/radtts/radtts.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, mel, speaker_ids, text, in_lens, out_lens, binarize_attention, attn_prior, f0, energy_avg, voiced_mask, p_voiced)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;31m# import pdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;31m# pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mtext_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mlog_s_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_det_W_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_mel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/src/app/radtts/radtts.py\u001b[0m in \u001b[0;36mencode_text\u001b[0;34m(self, text, in_lens)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mtext_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0mtext_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'@autocast() decorator is not supported in script mode'\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/src/app/radtts/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, in_lens)\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mcurr_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_ind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mb_ind\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0min_lens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                     curr_x = F.dropout(F.relu(conv(curr_x)),\n\u001b[0m\u001b[1;32m    354\u001b[0m                                        0.5, self.training)\n\u001b[1;32m    355\u001b[0m                 \u001b[0mx_embedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/src/app/radtts/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, signal, mask)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_partial_padding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mconv_signal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mconv_signal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/src/app/radtts/partialconv1d.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, mask_in)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 self.update_mask = F.conv1d(mask, self.weight_maskUpdater,\n\u001b[0m\u001b[1;32m     52\u001b[0m                                             \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR"
     ]
    }
   ],
   "source": [
    "# ================ MAIN TRAINNIG LOOP! ===================\n",
    "for epoch in range(epoch_offset, epochs):\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        print(i)\n",
    "        print(batch['audiopaths'])\n",
    "        tic = timer()\n",
    "        model.zero_grad()\n",
    "        (mel, speaker_ids, text, in_lens, out_lens, attn_prior,\n",
    "         f0, voiced_mask, p_voiced, energy_avg,\n",
    "         audiopaths) = parse_data_from_batch(batch)\n",
    "\n",
    "        if iteration >= binarization_start_iter:\n",
    "            binarize = True   # binarization training phase\n",
    "        else:\n",
    "            binarize = False  # no binarization, soft alignments only\n",
    "\n",
    "        with amp.autocast(use_amp):\n",
    "            \n",
    "            outputs = model(\n",
    "                mel, speaker_ids, text, in_lens, out_lens,\n",
    "                binarize_attention=binarize, attn_prior=attn_prior,\n",
    "                f0=f0, energy_avg=energy_avg,\n",
    "                voiced_mask=voiced_mask, p_voiced=p_voiced)\n",
    "            print(text)\n",
    "            loss_outputs = criterion(outputs, in_lens, out_lens)\n",
    "\n",
    "            loss = None\n",
    "            for k, (v, w) in loss_outputs.items():\n",
    "                if w > 0:\n",
    "                    loss = v * w if loss is None else loss + v * w\n",
    "\n",
    "            w_bin = criterion.loss_weights.get('binarization_loss_weight', 1.0)\n",
    "            if binarize and iteration >= kl_loss_start_iter:\n",
    "                binarization_loss = attention_kl_loss(\n",
    "                    outputs['attn'], outputs['attn_soft'])\n",
    "                loss += binarization_loss * w_bin\n",
    "            else:\n",
    "                binarization_loss = torch.zeros_like(loss)\n",
    "            loss_outputs['binarization_loss'] = (binarization_loss, w_bin)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        if grad_clip_val > 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), grad_clip_val)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        toc = timer()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print_list = [\"iter: {}  ({:.2f} s)  |  lr: {}\".format(\n",
    "            iteration, toc-tic, current_lr)]\n",
    "\n",
    "        for k, (v, w) in loss_outputs.items():\n",
    "            reduced_v = reduce_tensor(v, n_gpus, 0).item()\n",
    "            loss_outputs[k] = reduced_v\n",
    "            if rank == 0:\n",
    "                print_list.append('  |  {}: {:.3f}'.format(k, v))\n",
    "                logger.add_scalar('train/'+k, reduced_v, iteration)\n",
    "\n",
    "        if rank == 0:\n",
    "            print(''.join(print_list), flush=True)\n",
    "\n",
    "        if iteration > -1 and iteration % iters_per_checkpoint == 0:\n",
    "\n",
    "            val_loss_outputs = compute_validation_loss(\n",
    "                iteration, model, criterion, valset, collate_fn,\n",
    "                batch_size, n_gpus, logger=logger,\n",
    "                train_config=train_config)\n",
    "            checkpoint_path = \"{}/model_{}\".format(\n",
    "                output_directory, iteration)\n",
    "            save_checkpoint(model, optimizer, learning_rate, iteration,\n",
    "                            checkpoint_path)\n",
    "            print('Validation loss:', val_loss_outputs)\n",
    "\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "# train(n_gpus, rank, **train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "522e0208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/src/app/radtts/data/lj_data/LJSpeech-1.1/wavs/LJ003-0277_22k_normalized.wav',\n",
       " '/usr/src/app/radtts/data/lj_data/LJSpeech-1.1/wavs/LJ045-0070_22k_normalized.wav',\n",
       " '/usr/src/app/radtts/data/lj_data/LJSpeech-1.1/wavs/LJ031-0015_22k_normalized.wav',\n",
       " '/usr/src/app/radtts/data/dataset-1675114471-bertie (2)/dataset/wavs/178_22k_normalized.wav']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['audiopaths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4979db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b930361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d4ba67d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/usr/src/app/radtts/radtts.py\u001b[0m(376)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    374 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    375 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 376 \u001b[0;31m        \u001b[0mtext_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    377 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    378 \u001b[0;31m        \u001b[0mlog_s_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_det_W_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_mel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> text_enc, text_embeddings = self.encode_text(text, in_lens)\n",
      "*** RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR\n",
      "ipdb> text\n",
      "tensor([[         0,          0,          0,          0,          0,          1,\n",
      "                  0,          0,          0,          0,         71,          1,\n",
      "         352947544326148, 4569845219330,         -1,       1201, 94892560096576, 94893722671536,\n",
      "                  0,          0, 94893722671856,          0, 94893716654848, 94892537219376,\n",
      "         140135632608528,          0,          0,          0,          0,          0,\n",
      "                  0,          1,          0,          0,          0,          0,\n",
      "                 72,          1, 352947544326148, 4569845219330,          0,        993,\n",
      "         94892560096576, 94893722671840,          1,          0,         48,        208,\n",
      "         140139087139720,          0,          1, 94893717957040, 94893722672160,          0,\n",
      "         94893716654848, 94892537219376, 140135632608608,          0,          0,          0,\n",
      "                  0,          0,          0,          1,          0,          0,\n",
      "                  0,          0,         73,          1, 352947544326148, 4569845219330,\n",
      "         140134523103280,        737, 94892560096576, 94893722672144,          1, 94888712470528,\n",
      "                 48,        208, 140139087139720,          0,          1, 94893717957040,\n",
      "         94893722672464,          0, 94893716654848, 94892537219376, 140135632608688,          0,\n",
      "                  0,          0,          0,          0,          0,          1,\n",
      "                  0,          0,          0,          0,         74,          1,\n",
      "         352947544326148, 4569845219330, 140135632580151,        481, 94892560096576, 94893722672448,\n",
      "                  1, 94888712470528,         48,        208, 140139087139720,          0,\n",
      "                  1, 94893717957040, 94893722672768,          0, 94893716654848, 94892537219376,\n",
      "         140135632608768,          0,          0,          0,          0,          0,\n",
      "                  0,          1,          0,          0,          0,          0,\n",
      "                 75,          1, 352947544326148, 4569845219330, 140134523115280,        225,\n",
      "         140139228433584, 140139228433584,          1, 140131897966592],\n",
      "        [94892459818272, 94892459818272, 94892459818272, 94892459818272, 94892459820544, 94892459818304,\n",
      "         140134522305520, 140134522305456, 94892459818240, 140134522305392, 140134522305584, 140134522305488,\n",
      "         94892459818272, 94892459818272, 140134522305808, 94892459818272, 140134522305328, 140134522305552,\n",
      "         140134522305616, 94892459818272, 94892459818272, 94892459818272, 94892459818272, 94892459818272,\n",
      "         94892459818272, 94892459818304, 94892459818272, 94892459818272, 94892459818272, 94892459818272,\n",
      "         94892459820576, 94892459818304, 140134522305264, 140134522305232, 94892459818272, 140134522305200,\n",
      "         140134522305168, 140134522305136, 94892459818304, 94892459818272, 94892459819808, 94892459824928,\n",
      "         140134522305104, 94892459818272, 94892459818304, 140134522305072, 140134522305040, 94892459818272,\n",
      "         140134522305008, 140134522304976, 140134522304944, 94892459818272, 94892459818272, 94892459818272,\n",
      "         94892459818272, 94892459818272, 94892459818272, 94892459818304, 94892459818272, 94892459818272,\n",
      "         94892459818272, 94892459818272, 94892459820608, 94892459818304, 140134522304912, 140134522304880,\n",
      "         140134522304848, 140134522304816, 140134522304784, 140134522304752, 94892459818304, 140134522304720,\n",
      "         94892459819808, 94892459824928, 140134522304688, 94892459818272, 94892459818304, 140134522304656,\n",
      "         140134522304624, 94892459818272, 140134522304592, 140134522304560, 140134522304528, 94892459818272,\n",
      "         94892459818272, 94892459818272, 94892459818272, 94892459818272, 94892459818272, 94892459818304,\n",
      "         94892459818272, 94892459818272, 94892459818272, 94892459818272, 94892459820640, 94892459818304,\n",
      "         140134522304496, 140134522304464, 140134522304432, 140134522304400, 140134522304368, 140134522304336,\n",
      "         94892459818304, 140134522304304, 94892459819808, 94892459824928, 140134522304272, 94892459818272,\n",
      "         94892459818304, 140134522304240, 140134522304208, 94892459818272, 140134522304176, 140134522304144,\n",
      "         140134522304112, 94892459818272, 94892459818272, 94892459818272, 94892459818272, 94892459818272,\n",
      "         94892459818272, 94892459818304, 94892459818272, 94892459818272, 94892459818272, 94892459818272,\n",
      "         94892459820672, 94892459818304, 140134522304048, 140134522303984, 140134522303952, 94892459825472,\n",
      "         140134522303888, 140134522303824, 94892459818304, 140134522304080,          0,        273,\n",
      "         94892604615680, 140139228433632,          0,          0],\n",
      "        [140139228434464, 140139228434464, 94892560096624, 94892560096624, 140134523334256, 140134523333744,\n",
      "         140134523333808, 140134523334512, 140134523330864, 140134523333872, 140134523334000, 140134523334448,\n",
      "         140134523334384, 140134523333616, 140132594269040, 140132594268976, 140135845932400, 140135622610032,\n",
      "         140135616420080, 140135616433776, 140135616433840, 140132594197168, 140135736760624, 140134523091760,\n",
      "         140134523089840, 140134523091568, 140134523089648, 140134523090288, 140134523092080, 140134523089584,\n",
      "         140134523091632, 140134523092656, 140134523092720, 140134523092784, 140134523092912, 140135616304880,\n",
      "         140135616305520, 140135616305584, 140135616305200, 140135616305840, 140135616305648, 140134523406768,\n",
      "         140132594313200, 140134523045424, 140134523045360, 140134523045872, 140134523044400, 140134523047856,\n",
      "         140134523044720, 140134523046896, 140134523044336, 140134523047216, 140134523046576, 140134523047408,\n",
      "         140134523045552, 140134523045488, 140134523043952, 140134523044208, 140134523044272, 140134523044784,\n",
      "         140134523044976, 140134523045296, 140134523044528, 140134523044912, 140134523044656, 140134523044848,\n",
      "         140134523047536, 140134523348208, 140134523347952, 140134523347440, 140134523348912, 140134523349680,\n",
      "         140134523349616, 140134523350256, 140134523349232, 140134523348976, 140134523350128, 140134523349360,\n",
      "         140134523349168, 140134523349104, 140134523347120, 140134523350384, 140134523347824, 140134523347568,\n",
      "         140134523350640, 140134523348400, 140134523349744, 140134523350768, 140134523350000, 140134523349936,\n",
      "         140134523350512, 140134523350832, 140134523350704, 140134523350192, 140134523350576, 140134523350448,\n",
      "         140134523350320, 140134523350064, 140134523350960, 140134523347376, 140134523347632, 140134523348272,\n",
      "         140134523349552, 140134523348592, 140134523347184, 140134523347504, 140134523348848, 140134523348080,\n",
      "         140134523349424, 140134523349872, 140134523348464, 140134523347696, 140134523350896, 140134523348336,\n",
      "         140134523348720, 140134523348528, 140134523348656, 140134523347888, 140134523346992, 140134523349040,\n",
      "         140134523349488, 140134523349808, 140134523348784, 140134523352368, 140134523352688, 140134523353008,\n",
      "         140134523353264, 140134523355056, 140134523351280, 140134523351152, 140134523352240, 140134523352752,\n",
      "         140134523351408, 140134523352880, 140134523352112, 140134523353072, 140134523352432, 140134523352624,\n",
      "         140134523351664, 140134523351536, 140134523353328, 140134523353136],\n",
      "        [140139228434464, 140139228434464, 94892560096624, 94892560096624, 140134523334256, 140134523333808,\n",
      "         140134523334512, 140134523330864, 140134523333872, 140134523333744, 140134523334000, 140134523334448,\n",
      "         140134523334384, 140134523333616, 140132594269040, 140132594268976, 140135845932400, 140135622610032,\n",
      "         140135616420080, 140135616433776, 140135616433840, 140132594197168, 140135736760624, 140134523091760,\n",
      "         140134523089840, 140134523091568, 140134523089648, 140134523090288, 140134523092080, 140134523089584,\n",
      "         140134523091632, 140134523092656, 140134523092720, 140134523092784, 140134523092912, 140135616304880,\n",
      "         140135616305520, 140135616305584, 140135616305200, 140135616305840, 140135616305648, 140134523406768,\n",
      "         140132594313200, 140134523045424, 140134523045360, 140134523045872, 140134523044400, 140134523047856,\n",
      "         140134523044720, 140134523046896, 140134523044336, 140134523047216, 140134523046576, 140134523047408,\n",
      "         140134523045552, 140134523045488, 140134523043952, 140134523044208, 140134523044272, 140134523044784,\n",
      "         140134523044976, 140134523045296, 140134523044528, 140134523044912, 140134523044656, 140134523044848,\n",
      "         140134523047536, 140134523348208, 140134523347952, 140134523347440, 140134523348912, 140134523349680,\n",
      "         140134523349616, 140134523350256, 140134523349232, 140134523348976, 140134523350128, 140134523349360,\n",
      "         140134523349168, 140134523349104, 140134523347120, 140134523350384, 140134523347824, 140134523347568,\n",
      "         140134523350640, 140134523348400, 140134523349744, 140134523350768, 140134523350000, 140134523349936,\n",
      "         140134523350512, 140134523350832, 140134523350704, 140134523350192, 140134523350576, 140134523350448,\n",
      "         140134523350320, 140134523350064, 140134523350960, 140134523347376, 140134523347632, 140134523348272,\n",
      "         140134523349552, 140134523348592, 140134523347184, 140134523347504, 140134523348848, 140134523348080,\n",
      "         140134523349424, 140134523349872, 140134523348464, 140134523347696, 140134523350896, 140134523348336,\n",
      "         140134523348720, 140134523348528, 140134523348656, 140134523347888, 140134523346992, 140134523349040,\n",
      "         140134523349488, 140134523349808, 140134523348784, 140134523352368, 140134523352688, 140134523353008,\n",
      "         140134523353264, 140134523355056, 140134523351280, 140134523351152, 140134523352240, 140134523352752,\n",
      "         140134523351408, 140134523352880, 140134523352112, 140134523353072, 140134523352432, 140134523352624,\n",
      "         140134523351664, 140134523351536, 140134523353328, 140134523353136]],\n",
      "       device='cuda:1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_INTERNAL_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-95ef23b34ae8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m outputs = model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_lens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbinarize_attention\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_prior\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mf0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy_avg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menergy_avg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     voiced_mask=voiced_mask, p_voiced=p_voiced)\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/src/app/radtts/radtts.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, mel, speaker_ids, text, in_lens, out_lens, binarize_attention, attn_prior, f0, energy_avg, voiced_mask, p_voiced)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mtext_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mlog_s_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_det_W_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_mel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/src/app/radtts/radtts.py\u001b[0m in \u001b[0;36mencode_text\u001b[0;34m(self, text, in_lens)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mtext_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0mtext_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'@autocast() decorator is not supported in script mode'\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/src/app/radtts/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, in_lens)\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mcurr_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_ind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mb_ind\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0min_lens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                     curr_x = F.dropout(F.relu(conv(curr_x)),\n\u001b[0m\u001b[1;32m    354\u001b[0m                                        0.5, self.training)\n\u001b[1;32m    355\u001b[0m                 \u001b[0mx_embedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/src/app/radtts/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, signal, mask)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_partial_padding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mconv_signal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mconv_signal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/src/app/radtts/partialconv1d.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, mask_in)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         raw_out = super(PartialConv1d, self).forward(\n\u001b[0m\u001b[1;32m     60\u001b[0m             torch.mul(input, mask) if mask_in is not None else input)\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    307\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m--> 309\u001b[0;31m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    310\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR"
     ]
    }
   ],
   "source": [
    "            outputs = model(\n",
    "                mel, speaker_ids, text, in_lens, out_lens,\n",
    "                binarize_attention=binarize, attn_prior=attn_prior,\n",
    "                f0=f0, energy_avg=energy_avg,\n",
    "                voiced_mask=voiced_mask, p_voiced=p_voiced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "830b4605",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-a8f85c3ddf3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m (mel, speaker_ids, text, in_lens, out_lens, attn_prior,\n\u001b[1;32m      2\u001b[0m  \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoiced_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_voiced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy_avg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m  audiopaths) = parse_data_from_batch(batch)\n\u001b[0m",
      "\u001b[0;32m/usr/src/app/radtts/train.py\u001b[0m in \u001b[0;36mparse_data_from_batch\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0maudiopaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'audiopaths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mattn_prior\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mattn_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_prior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf0\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "            (mel, speaker_ids, text, in_lens, out_lens, attn_prior,\n",
    "             f0, voiced_mask, p_voiced, energy_avg,\n",
    "             audiopaths) = parse_data_from_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "429a6721",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "numel: integer multiplication overflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    392\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                                 \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                             \u001b[0;32mreturn\u001b[0m \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    425\u001b[0m             )\n\u001b[1;32m    426\u001b[0m         \u001b[0;31m# All strings are unicode in Python 3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     def backward(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0mguard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DisableFuncTorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_str_intern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    566\u001b[0m                         \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m                         \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    326\u001b[0m         )\n\u001b[1;32m    327\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             nonzero_finite_vals = torch.masked_select(\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0mtensor_view\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mtensor_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: numel: integer multiplication overflow"
     ]
    }
   ],
   "source": [
    "import os\n",
    "[os.path.getsize(filepath) for filepath in batch['audiopaths']]\n",
    "# attn_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b0d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f00b9ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mel', 'speaker_ids', 'text', 'input_lengths', 'output_lengths', 'audiopaths', 'attn_prior', 'f0', 'p_voiced', 'voiced_mask', 'energy_avg'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b4ff05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/src/app/radtts/data/lj_data/LJSpeech-1.1/wavs/LJ033-0161_22k_normalized.wav',\n",
       " '/usr/src/app/radtts/data/lj_data/LJSpeech-1.1/wavs/LJ019-0312_22k_normalized.wav',\n",
       " '/usr/src/app/radtts/data/eminem/wav22050/261_22k_normalized.wav',\n",
       " '/usr/src/app/radtts/data/Carolyn_Singing/wavs/261_22k_normalized.wav']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['audiopaths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d4125b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  8,  63,  88,   8,  94,  82,  79,   8, 127, 139,   8,  89,  80,   8,\n",
       "          94,  82,  79,   8, 110, 168, 108, 168, 110, 156, 139, 169, 110, 156,\n",
       "           3,   8,  94,  82,  79,   8, 127, 107, 154, 110, 168,   8, 166, 110,\n",
       "         154, 150, 168,   8, 110, 125, 170, 139, 156, 127,   8,  75,   8, 168,\n",
       "         107, 155, 166, 110, 154,   8,  89,  80,   8, 167, 107, 166, 145, 157,\n",
       "           8, 166, 139, 166, 134,   8,  75,  88,  78,   8, 170, 139, 166,   8],\n",
       "        [  8,  97,  83,  94,  82,   8,  94,  82,  79,   8, 154, 160, 153, 110,\n",
       "         154,   8, 152, 175, 167, 145, 168, 127, 146, 153, 169, 110, 156, 183,\n",
       "           3,   8, 116, 154, 128, 160,   8, 168, 170, 146, 154,   8, 180, 131,\n",
       "         167, 149,   8, 154, 150, 156, 182, 110, 156, 170, 154, 149,   8, 127,\n",
       "         145, 168, 166, 160, 183, 127,   4,   8,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  8, 123, 154,   8, 166, 167, 103, 125, 110, 125, 154, 151,   8, 156,\n",
       "         131, 180, 134,   8,  81,  79,  94,   8,  94,  82,  79,   8, 166, 167,\n",
       "         103, 166, 168,   8, 123,   8, 141, 150, 154,   8, 123,   8, 131, 180,\n",
       "         134,   8, 127, 145, 183, 135, 180,   4,   8,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  8, 115, 154,   8, 123,   8,  78,  89,   4,   8,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07b90207",
   "metadata": {},
   "outputs": [],
   "source": [
    "text  = batch['text'].cuda()\n",
    "in_lens = batch['input_lengths'].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db79eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths, ids_sorted_decreasing = torch.sort(\n",
    "    torch.LongTensor([len(x[\"text_encoded\"]) for x in batch]),\n",
    "    dim=0,\n",
    "    descending=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
